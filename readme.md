This is an attempt to create a extrapolating autoregressive decoder only mixture of experts transformer that uses recurrent blocks, with rotary postional embeddings, And post layer normalization

[colab](https://colab.research.google.com/drive/1HSPsHApii0XUeX6N7MAC1P5tSS4ZfCv0?usp=sharing)  
  
  
  
[Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach](https://arxiv.org/abs/2502.05171)  
[End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking](https://arxiv.org/abs/2202.05826)  
[Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models](https://arxiv.org/abs/2405.14297)  
[The Curse of Depth in Large Language Models](https://www.arxiv.org/abs/2502.05795)  
[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)  
[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)  
